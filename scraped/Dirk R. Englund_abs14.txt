
Artificial Neural Networks are computational network models inspired by
signal processing in the brain. These models have dramatically improved the
performance of many learning tasks, including speech and object recognition.
However, today's computing hardware is inefficient at implementing neural
networks, in large part because much of it was designed for von Neumann
computing schemes. Significant effort has been made to develop electronic
architectures tuned to implement artificial neural networks that improve upon
both computational speed and energy efficiency. Here, we propose a new
architecture for a fully-optical neural network that, using unique advantages
of optics, promises a computational speed enhancement of at least two orders of
magnitude over the state-of-the-art and three orders of magnitude in power
efficiency for conventional learning tasks. We experimentally demonstrate
essential parts of our architecture using a programmable nanophotonic
processor.
