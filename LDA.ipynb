{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation on MIT projects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First a simple example\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named gensim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-316060e95eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Preparing Document-Term Matrix containing (term ID, term frequency) pairs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Creating the term dictionary of our courpus, where every unique term is assigned an index.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named gensim"
     ]
    }
   ],
   "source": [
    "# corpus\n",
    "doc1 = \"Music is good to hear because it can make you happy and you can play it too\"\n",
    "doc2 = \"it's bad working all your life and have no time for love\"\n",
    "doc3 = \"Enjoying music is good and relaxing. it is best to play it but you need concentration\"\n",
    "doc4 = \"I program easy stuff. I enjoy it staying in front of the computer all the time\"\n",
    "doc5 = \"politics is corrupt, this is a rigged system. My favorite dictator would put everything in order\"\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "\n",
    "# Tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "#from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "#lemma = WordNetLemmatizer()\n",
    "pstemmer = PorterStemmer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    normalized = \" \".join(pstemmer.stem(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]\n",
    "\n",
    "# Preparing Document-Term Matrix containing (term ID, term frequency) pairs\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "doc_clean[0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(dictionary.token2id['music'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "topics = ldamodel.print_topics(num_topics=3, num_words=3)\n",
    "for t in topics: print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now back to the MIT abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#Import dictionary with urls from file\n",
    "with open('abs_dict.json', 'r') as outfile:\n",
    "    abs_dict = json.load(outfile)\n",
    "#If the file is not there get it from the script\n",
    "if 'abs_dict' not in locals():\n",
    "    import gather\n",
    "    abs_dict = gather.main()\n",
    "    with open('abs_dict.json', 'w') as outfile:\n",
    "        json.dump(abs_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read the abstracts that I previously scraped and stored in the /abstract/ folder (find more details in README.md)\n",
    "import scrape\n",
    "corpus = scrape.readAbstracts()\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create documents list with tokenized text\n",
    "# Tokenize\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "import re\n",
    "stop = set(stopwords.words('english')) #NB a set is more efficient than a list\n",
    "exclude = set(string.punctuation) \n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "#pstemmer = PorterStemmer()\n",
    "def clean_text(doc, skipWords=[]):\n",
    "    for w in skipWords:\n",
    "        doc = doc.replace(w,'')\n",
    "    #Remove non-letters and convert to lower case\n",
    "    #Remove non unicode (accents etc). Remove word that can starts with non letter and ends with non letter. \n",
    "    #\"1st\" \"a2\" \"1a3\" safe, \"1\" \"123\" esta' \" not\n",
    "    letters_only = re.sub(r'\\W|\\b\\w*\\d\\b', ' ', doc.lower())\n",
    "    #Split into individual words and remove stop words\n",
    "    #This can be done in TfidfVectorizer/CountVectorizer but ok..\n",
    "    stop_free = \" \".join([i for i in letters_only.split() if i not in stop])\n",
    "    #Remove punctuations should not be needed after letters_only but who knows\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #Stemming/Lemmatizing\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "documents = [clean_text(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit [LDA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)\n",
    "I am using 2-grams (i.e. features composed of two words). Five topics (N=5) can be discerned using a quite low cutoff for the words appearing in more than 95% of the documets (max_df=0.95). \n",
    "\n",
    "I played with the parameters and the topics seem relatively stable. Based on the pyLDAvis plot, using 1-grams leads to two partially overlapping topics with N=5. This is the reason why I chose 2-grams, although I only see 2-gram (\"lower bound\") within the most salient features. The fit using for 1- and 2-grams are sensitive on the maximum number of features (here max_features=1000) but seem stable when changing max_df from 50% to 95%. The fit with 1-grams on the other hand needs lower value for max_features to show a better qualitative fit (i.e. less overlap in topics).\n",
    "\n",
    "\n",
    "<!--2-grams: 5 topics, max_features=1000 is well separated. a few are similar to the pdf. better than 1-gram. I see one 2-gram only (\"lower bound\"). max_features=1700 begins to overlap. max_df=0.95, 0.75, 0.5 have very similar fit\n",
    "\n",
    "1-gram: 5 topics, max_features=700 is better than 1000, though overlap 2 topics with both max_df=0.95 and 0.65\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Learn the vocabulary dictionary and return term-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(analyzer='word', stop_words='english', #token_pattern = r'\\b[a-zA-Z]{3,}\\b',#strip_accents='ascii', tokenizer=lemma.lemmatize, lowercase=True, \n",
    "                        ngram_range=(1,2), max_df=0.95, min_df=5, max_features=1000)\n",
    "#Fit Transform - Learn the vocabulary dictionary and return term-document matrix\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "no_topics = 5\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=0, n_jobs=2)\n",
    "lda.fit(tf)\n",
    "# Display the terms in the topics\n",
    "#tf_vectorizer.get_feature_names(): map feature indices -> feature name\n",
    "#lda.components_[i,j] (topic word distribution): \"weights\" of terms j in topic i\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for ind, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (ind)\n",
    "        print \" \".join([feature_names[i]+' -'\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "print\n",
    "print \"LDA:\"\n",
    "display_topics(lda, tf_feature_names, no_top_words = 10)\n",
    "#Plot with pyLDAvis - see http://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/sklearn.ipynb\n",
    "import pyLDAvis.sklearn, pyLDAvis\n",
    "pyLDAvis.enable_notebook();\n",
    "pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "def plot_wordcloud(components,feature_names,numbest = 25):\n",
    "    for t in range(len(components)):\n",
    "        best_terms = dict([(feature_names[i],components[t,i]) for i in components[t].argsort()[:-numbest-1:-1].tolist()])\n",
    "        # Generate a word cloud image\n",
    "        wordcloud = WordCloud(width=400, height=200, margin=20, background_color='black', max_font_size=50) #random_state=i, \n",
    "        wordcloud.generate_from_frequencies(best_terms)\n",
    "        # Display the generated image:\n",
    "        # the matplotlib way:\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "plot_wordcloud(lda.components_,tf_feature_names,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preparing Document-Term Matrix\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus]\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=5, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics(num_topics=5, num_words=7)\n",
    "for t in topics: print t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
